{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwzUNl7RrjHf"
   },
   "source": [
    "# One model, one currency pair, all study periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99e5994552978071463d487aad685a1195778804",
    "colab_type": "text",
    "id": "okTqNAzZrjHg"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "77Z6HnU1rjHh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras_sequential_ascii import keras2ascii \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, LSTM, GRU, Dropout\n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cfcc894ec0e93e6d4bb79eeefb92f300213bfb52",
    "colab_type": "text",
    "id": "Dn5SarxdrjHn"
   },
   "source": [
    "### Display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "mM_BX6p_rjHo"
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "pd.set_option(\"display.max_rows\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2bedcf4277e569066f92fc4fdf077b3709f50aca",
    "colab_type": "text",
    "id": "e7t4ByttrjHr"
   },
   "source": [
    "## Function defintions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18098a7b56efc725e0881daa6e5313f44e2e3ffc",
    "colab_type": "text",
    "id": "wqfgNMpDrjHt"
   },
   "source": [
    "### Data preprocessing\n",
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4SUZtNTrjHt"
   },
   "outputs": [],
   "source": [
    "def scale_data(time_series, scaler, train_len):\n",
    "    \"\"\"\n",
    "    Scales a time series.\n",
    "    Scaler options: from sklearn.preprocessing: \n",
    "    MinMaxScaler (e.g. ranges(0,1), (-1,1)), StandardScaler (with or w/o std)\n",
    "    Splits time series into training and trading (=test) data. \n",
    "    Scaler is fitted only to training data, \n",
    "    transformation is applied to the whole data set.\n",
    "    Returns the stitched train and trade data \n",
    "    and the scaler fitted on the train data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create train and trade set\n",
    "    train = time_series[:train_len]\n",
    "    trade = time_series[train_len:]\n",
    "    \n",
    "    # scale from train set only\n",
    "    # reshape to be usable with scaler\n",
    "    train = train.values.reshape(train.shape[0], 1)\n",
    "    trade = trade.values.reshape(trade.shape[0], 1)\n",
    "    # fit scaler to training set only\n",
    "    fitted_scaler = scaler.fit(train)  \n",
    "    # scale both sets with the training set scaler\n",
    "    train = fitted_scaler.transform(train)\n",
    "    trade = fitted_scaler.transform(trade)\n",
    "    # inverse transformation \n",
    "    # fitted_scaler.inverse_transform(train_z)\n",
    "    # fitted_scaler.inverse_transform(trade_z)\n",
    "    \n",
    "    stitched = np.concatenate((train, trade), axis=0)\n",
    "    \n",
    "    return stitched, fitted_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a00ed7c591415a8f4698b2e3e86fb412a8c1ec88",
    "colab_type": "text",
    "id": "-Cu0iQFxrjHy"
   },
   "source": [
    "#### Input sequence creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_UGWO6XrjHy"
   },
   "outputs": [],
   "source": [
    "def create_input_sequences(time_series, fitted_scaler, train_len, seq_len, \n",
    "                           targets=\"classification_1D\", pred_steps=1):\n",
    "    \"\"\"\n",
    "    Converts a time series to a supervised problem for recurrent neural networs: \n",
    "    Creates the X's (windows of length seq_len)\n",
    "    and the respective y's (the observation pred_steps steps after the windows)\n",
    "    for both the train and trade set.\n",
    "    targets: \"regression\", \"classification_1D\" (sparse, value in [0, 1]), \n",
    "    \"classification_2D\" (one-hot encoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    all_windows = []  # empty time_series for training windows\n",
    "    for i in range(len(time_series) - seq_len):  \n",
    "        all_windows.append(time_series[i : (i + seq_len) + pred_steps])  \n",
    "        # we split the windows up into (X, y) later, + pred_steps are the y's\n",
    "        \n",
    "    all_windows = np.array(all_windows)  # make it a numpy array\n",
    "    # number of all windows = len(time_series) - seq_len\n",
    "    \n",
    "    split_at_row = int(train_len - seq_len)\n",
    "    \n",
    "    # train windows\n",
    "    train_windows = all_windows[:split_at_row, :]\n",
    "#     np.random.shuffle(train_windows)  \n",
    "# keeps the windows intact, but shuffles their order\n",
    "    \n",
    "    x_train = train_windows[:, :-1]\n",
    "    \n",
    "    y_train = train_windows[:, -1]  # scaled returns\n",
    "    if \"classification\" in targets:\n",
    "      # one-hot encoding: col 0: returns < 0, col 1: returns >= 0\n",
    "        y_train = to_categorical(fitted_scaler.inverse_transform(y_train) >= 0)  \n",
    "        if targets == \"classification_1D\":\n",
    "          # if real returns >= 0: 1, else: 0 (only take col 1)\n",
    "            y_train = y_train[:, 1]  \n",
    "    \n",
    "    # trade windows\n",
    "    trade_windows = all_windows[split_at_row:, :]\n",
    "    \n",
    "    x_trade = trade_windows[:, :-1]\n",
    "    \n",
    "    y_trade = trade_windows[:, -1]\n",
    "    if \"classification\" in targets:\n",
    "       # one-hot encoding: col 0: returns < 0, col 1: returns >= 0\n",
    "        y_trade = to_categorical(fitted_scaler.inverse_transform(y_trade) >= 0) \n",
    "        if targets == \"classification_1D\":\n",
    "          # if real returns >= 0: 1, else: 0 (only take col 1)\n",
    "            y_trade = y_trade[:, 1]  \n",
    "\n",
    "    # reshape seq.s into 3D: (samples, sequence_length/time steps, features)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) \n",
    "    x_trade = np.reshape(x_trade, (x_trade.shape[0], x_trade.shape[1], 1))  \n",
    "\n",
    "    return [x_train, y_train, x_trade, y_trade]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8nWegnodrjH4"
   },
   "source": [
    "#### Scaling and sequence creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NoX_ppNrjH5"
   },
   "outputs": [],
   "source": [
    "def data_prep(time_series, scaler, train_len, seq_len, \n",
    "              targets=\"classification_1D\", pred_steps=1):\n",
    "    \"\"\"\n",
    "    Data preparation:\n",
    "    Scaling, then creating input sequences for supervised learning.\n",
    "    \"\"\"\n",
    "    # Scale time series, return scaled ts and scaler for inverse scaling\n",
    "    scaled_ts, fitted_scaler = scale_data(time_series=time_series,\n",
    "                                          scaler=scaler,\n",
    "                                          train_len=train_len)\n",
    "    \n",
    "    # Create input sequences and return inputs and targets for both training and trading data\n",
    "    x_train, y_train, x_trade, y_trade = create_input_sequences(time_series=scaled_ts,\n",
    "                                                                fitted_scaler=fitted_scaler, \n",
    "                                                                train_len=train_len,\n",
    "                                                                seq_len=seq_len, \n",
    "                                                                targets=targets, \n",
    "                                                                pred_steps=pred_steps)\n",
    "    \n",
    "    return [scaled_ts, fitted_scaler, x_train, y_train, x_trade, y_trade]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c19e353c6b39bab25e4bb611e834b284e34d00e1",
    "colab_type": "text",
    "id": "c0bR7U7prjH9"
   },
   "source": [
    "### Model building functions\n",
    "#### Feedforward network (Multi-layer perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78a0676a2918a2260e2217f1cff3f73fc82f5fd8",
    "colab": {},
    "colab_type": "code",
    "id": "f-X0bjy1rjH9"
   },
   "outputs": [],
   "source": [
    "def build_fnn(input_dim, hidden_layers, neurons, dropout, loss, \n",
    "              output_activation, optimizer='adam', summary=False):\n",
    "    \"\"\"\n",
    "    Builds a feedforward neural network model for binary classification.\n",
    "    input_dim: number of observations (for comparison with RNNs: sequence length)\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # input dropout\n",
    "    model.add(Dropout(dropout))\n",
    "        \n",
    "    # first hidden layer\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # hidden layers in between\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1, activation=output_activation))\n",
    "\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['binary_accuracy'])\n",
    "    \n",
    "    # print summary of layers and parameters\n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-0qu8a1rjID"
   },
   "source": [
    "#### Recurrent Neural Networks (Simple RNN, LSTM, GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78a0676a2918a2260e2217f1cff3f73fc82f5fd8",
    "colab": {},
    "colab_type": "code",
    "id": "dEg9ri6XrjIF"
   },
   "outputs": [],
   "source": [
    "def build_rnn(rnn_type, input_shape, hidden_layers, neurons, dropout, loss, \n",
    "              output_activation, optimizer='adam', summary=False):\n",
    "    \"\"\"\n",
    "    Builds a recurrent neural network model for regression.\n",
    "    input_shape: (sequence length, number of features)\n",
    "    rnn_type: RNN, SimpleRNN, LSTM, GRU, CuDRNNLSTM, CuDRNNGRU, Bidirectional \n",
    "    (see https://keras.io/layers/recurrent/)\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # input dropout\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    if hidden_layers > 1:\n",
    "        # first hidden layer\n",
    "        model.add(rnn_type(neurons, \n",
    "                           input_shape=input_shape, \n",
    "                           return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        # hidden layers in between\n",
    "        for _ in range(hidden_layers - 2):\n",
    "            model.add(rnn_type(neurons, \n",
    "                               return_sequences=True))\n",
    "            model.add(Dropout(dropout))\n",
    "        \n",
    "        # final hidden layer before dense layer\n",
    "        model.add(rnn_type(neurons))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "    else:\n",
    "        # single hidden layer\n",
    "        model.add(rnn_type(neurons, \n",
    "                           input_shape=input_shape))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1, activation=output_activation))\n",
    "\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['binary_accuracy'])\n",
    "    \n",
    "    # print summary of layers and parameters\n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cq1ZbPPXrjII"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6FMUXb5rjIJ"
   },
   "outputs": [],
   "source": [
    "def training_one_period(model, x_train, y_train, batch_size, max_epochs=100, \n",
    "             val_split=0.2, verbose=1, patience=10):\n",
    "    \"\"\"\n",
    "    Takes a compiled model and trains it on training data.\n",
    "    \"\"\"\n",
    "    start_training = time.time()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                               patience=patience, \n",
    "                               verbose=verbose, \n",
    "                               mode='auto', \n",
    "                               restore_best_weights=True),\n",
    "                               PlotLossesKeras()]  #,\n",
    "#                  ModelCheckpoint(monitor='val_loss', \n",
    "#                                  filepath='weights/weights_e{epoch:02d}-vl{val_loss:.4f}.hdf5',\n",
    "#                                  verbose=verbose, \n",
    "#                                  save_best_only=True,\n",
    "#                                  save_weights_only=True)] \n",
    "\n",
    "    hist = model.fit(x_train, y_train, \n",
    "                     batch_size=batch_size, \n",
    "                     epochs=max_epochs,\n",
    "                     verbose=verbose,\n",
    "                     validation_split=val_split, \n",
    "                     callbacks=callbacks)\n",
    "    \n",
    "    training_time = time.time() - start_training\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"Time: {round(training_time/60)} minutes\")\n",
    "    \n",
    "    return model, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1547
    },
    "colab_type": "code",
    "id": "03Bc1PO3rjJE",
    "outputId": "c330d031-8e34-4698-c146-19a6bb153e14"
   },
   "outputs": [],
   "source": [
    "def training_all_periods(currency, hidden_layer_type, hidden_layers, neurons, dropout):\n",
    "    \"\"\"\n",
    "    Training function that enables to specify a model to train by the currency, the type \n",
    "    and number ofhidden layers, number of neurons per hidden layer and dropout rate.\n",
    "    ATTENTION:\n",
    "    This function was defined mainly for readability of the later training process\n",
    "    but makes use of variables defined outside the function and is not very generic.\n",
    "    Use only after defining the scaler, train_len, trade_len, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # timing\n",
    "    start_all = time.time()\n",
    "\n",
    "    # isolate currency pair\n",
    "    ts = returns[currency].dropna()\n",
    "\n",
    "    # determine number of study perios\n",
    "    study_periods = int((len(ts) - train_len) / trade_len)\n",
    "\n",
    "    # loop through all study periods\n",
    "    for period_no in reversed(range(study_periods)):\n",
    "\n",
    "        #### Data Preprocessing ####\n",
    "\n",
    "        # isolate study period\n",
    "        sp_stop = len(ts) - period_no * trade_len\n",
    "        sp_start = sp_stop - (train_len + trade_len)\n",
    "        time_series = ts[sp_start : sp_stop]\n",
    "\n",
    "        # data preparation: scaling and creating a supervised problem\n",
    "        scaled_ts, fitted_scaler, x_train, y_train, x_trade, y_trade = data_prep(time_series=time_series,\n",
    "                                                                                 scaler=scaler, \n",
    "                                                                                 train_len=train_len, \n",
    "                                                                                 seq_len=sequence_len, \n",
    "                                                                                 targets='classification_1D', \n",
    "                                                                                 pred_steps=1)\n",
    "\n",
    "        if hidden_layer_type == Dense:\n",
    "\n",
    "            #### FNN ####\n",
    "\n",
    "            # model building\n",
    "            model_to_train = build_fnn(input_dim=sequence_len,\n",
    "                                       hidden_layers=hidden_layers, \n",
    "                                       neurons=neurons, \n",
    "                                       dropout=dropout, \n",
    "                                       loss='binary_crossentropy', \n",
    "                                       output_activation='sigmoid', \n",
    "                                       optimizer='adam', \n",
    "                                       summary=False)\n",
    "\n",
    "            # flatten input arrays (FNNs don't use sequences as inputs)\n",
    "            x_train_flat = np.reshape(x_train, (x_train.shape[0], x_train.shape[1]))\n",
    "            # also flat, but generic name for generic evaluation function\n",
    "            x_trade = np.reshape(x_trade, (x_trade.shape[0], x_trade.shape[1]))\n",
    "\n",
    "            # model training\n",
    "            trained_model, training_time = training_one_period(model_to_train, \n",
    "                                                                 x_train_flat,  # flat!\n",
    "                                                                 y_train, \n",
    "                                                                 batch_size=batch_size, \n",
    "                                                                 max_epochs=max_epochs, \n",
    "                                                                 val_split=validation_split, \n",
    "                                                                 verbose=verbose, \n",
    "                                                                 patience=patience)\n",
    "\n",
    "        else:\n",
    "\n",
    "            #### Recurrent ####\n",
    "\n",
    "            # rnn model\n",
    "            model_to_train = build_rnn(rnn_type=hidden_layer_type,\n",
    "                                       input_shape=(sequence_len, 1),\n",
    "                                       hidden_layers=hidden_layers, \n",
    "                                       neurons=neurons, \n",
    "                                       dropout=dropout, \n",
    "                                       loss='binary_crossentropy', \n",
    "                                       output_activation='sigmoid', \n",
    "                                       optimizer='adam', \n",
    "                                       summary=False)\n",
    "\n",
    "            # model training    \n",
    "            trained_model, training_time = training_one_period(model_to_train, \n",
    "                                                                 x_train, \n",
    "                                                                 y_train, \n",
    "                                                                 batch_size=batch_size, \n",
    "                                                                 max_epochs=max_epochs, \n",
    "                                                                 val_split=validation_split, \n",
    "                                                                 verbose=verbose, \n",
    "                                                                 patience=patience)\n",
    "\n",
    "        if (period_no + 1) % 5 == 0:\n",
    "            print(f'{currency}, period {period_no + 1}/{study_periods}, {round(training_time/60, 1)} min')\n",
    "\n",
    "        # get: log_loss, accuracy, roc_auc, profits, sharpe ratio\n",
    "        results_this_model = list(evaluation(model=trained_model,\n",
    "                                             time_series=time_series,\n",
    "                                             x_true=x_trade,  # flat!\n",
    "                                             y_true=y_trade))\n",
    "\n",
    "        # append training time\n",
    "        results_this_model.append(training_time)\n",
    "\n",
    "        # write results into dataframe\n",
    "        for j in range(len(metrics)):\n",
    "                results_dict[currency].loc[(model_str, period_no + 1), metrics[j]] = results_this_model[j]\n",
    "\n",
    "\n",
    "    # total training time for all study periods\n",
    "    print(f'Done. Time: {round((time.time() - start_all)/60)} minutes')\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64d8d5e56cc3f083a22d32183054716ee6aeeb53",
    "colab_type": "text",
    "id": "bPPTrHigrjIM"
   },
   "source": [
    "### Evaluation\n",
    "#### Economic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "011fed6be34ec44272f65b470a84d4f69f3d04d2",
    "colab": {},
    "colab_type": "code",
    "id": "3yQnbXPhrjIN"
   },
   "outputs": [],
   "source": [
    "def trading_strategy(y_true_returns, y_pred, midpoint=0.5, threshold=0):\n",
    "    \"\"\"\n",
    "    Calculates cumulative absolute profits (i.e. p.a. profits for 250 days \n",
    "    of trading) from a simple trading strategy of going long when predicted \n",
    "    returns are on or above a midpoint + threshold (Default: 0.5 + 0) and \n",
    "    short when below midpoint - threshold.    \n",
    "    threshold = 0 (Default) means trading every prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    returns = []\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        \n",
    "        # if model predicts positive return,  go long\n",
    "        if y_pred[i] >= midpoint + threshold:\n",
    "            returns.append(y_true_returns[i])\n",
    "            \n",
    "        # else, go short\n",
    "        elif y_pred[i] < midpoint - threshold:\n",
    "            returns.append(0 - y_true_returns[i])\n",
    "    \n",
    "    profits = listproduct([(1 + r) for r in returns]) - 1\n",
    "    stdev = np.std(returns)\n",
    "    sharpe_ratio = np.mean(returns) / stdev\n",
    "    \n",
    "    return profits, stdev, sharpe_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sT6P7XX_rjIR"
   },
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e3F0jiWPrjIT"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, time_series, x_true, y_true):\n",
    "    \"\"\"\n",
    "    Evaluates a model's predictions.\n",
    "    \"\"\"\n",
    "    # get true returns (i.e. not the binary variable y_true) \n",
    "    # from time_series (before scaling)\n",
    "    y_trade_returns = time_series[-250:].values\n",
    "    \n",
    "    # predict y_trade\n",
    "    y_pred = model.predict(x_true, verbose=0)[:, 0]\n",
    "    \n",
    "    # profits: true returns, predicted probabilities\n",
    "    profits, stdev, sharpe_ratio = trading_strategy(y_trade_returns, y_pred)\n",
    "    \n",
    "    # log loss and accuracy: x_trade sequences, true binary labels\n",
    "    log_loss, accuracy = model.evaluate(x_true, y_true, verbose=0)\n",
    "    \n",
    "    # area under ROC curve: true binary labels, predicted probabilities\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "    return log_loss, accuracy, roc_auc, profits, stdev, sharpe_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JJzmR1yrjIY"
   },
   "source": [
    "### Helper functions\n",
    "#### Listproduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VivFhEo7rjIa"
   },
   "outputs": [],
   "source": [
    "def listproduct(lst):\n",
    "    \"\"\"\n",
    "    Takes a list argument and returns the \n",
    "    product of all its elements.\n",
    "    \"\"\"\n",
    "    product = 1\n",
    "    for number in lst:\n",
    "        product *= number\n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_d0uV4jrjIe"
   },
   "source": [
    "#### Multi-index dataframe for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UL94HdfrrjIg"
   },
   "outputs": [],
   "source": [
    "def create_results_df(study_periods, metrics, models=['FNN', 'SRNN', 'LSTM', 'GRU']):\n",
    "    \"\"\"\n",
    "    Returns a multi-index pd.DataFrame filled with '_' in each cell.\n",
    "    Columns: evaluation metrics\n",
    "    Row levels: models (level 0), study periods (level 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # multi-index\n",
    "    idx = pd.MultiIndex.from_product([models,  # for each model type\n",
    "                                     list(range(1, study_periods + 1))],  # one row per study period\n",
    "                                     names=['Model', 'Study period'])\n",
    "\n",
    "    # empty results dataframe \n",
    "    return pd.DataFrame('-', idx, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKA2dMoDrjIi"
   },
   "source": [
    "#### Dataframe aggregation by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "joDIYsAGrjIk"
   },
   "outputs": [],
   "source": [
    "def aggregate_results_by_model(granular_df, models=['FNN', 'SRNN', 'LSTM', 'GRU']):\n",
    "    \"\"\"\n",
    "    Aggregates the values in a granular dataframe \n",
    "    (data for every study period) by model.\n",
    "    \"\"\"\n",
    "    # aggregated per model results\n",
    "    aggregated_df = pd.DataFrame(columns=metrics)\n",
    "\n",
    "    for nn in models:\n",
    "        aggregated_df.loc[nn] = granular_df.loc[nn].mean(axis=0)\n",
    "\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99e5994552978071463d487aad685a1195778804",
    "colab_type": "text",
    "id": "QdHg5FIMrjIm"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c3278d331233899e0f1f399abed7b28ef243c54",
    "colab": {},
    "colab_type": "code",
    "id": "edUNpXEsrjIn"
   },
   "outputs": [],
   "source": [
    "dataset_raw = pd.read_csv('exchange_rates_FED.csv', header=3, index_col=0).iloc[2:,:]\n",
    "dataset_raw = dataset_raw.add_prefix('USD/')\n",
    "dataset_raw = dataset_raw.rename(index=str, columns={\"USD/USD\":\"EUR/USD\", \"USD/USD.1\":\"GBP/USD\", \"USD/USD.2\":\"AUD/USD\", \"USD/USD.3\":\"NZD/USD\",\n",
    "                                            \"USD/Unnamed: 19\":\"NBDI\", \"USD/Unnamed: 20\":\"NMCDI\", \"USD/Unnamed: 21\":\"NOITPI\"})\n",
    "\n",
    "dates = dataset_raw.index  # save index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdRpjQWKrjIr"
   },
   "outputs": [],
   "source": [
    "# to numeric\n",
    "prices = pd.DataFrame(columns=dataset_raw.columns)\n",
    "for col in prices.columns:\n",
    "    prices[col] = pd.to_numeric(dataset_raw[col].astype(str), errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e81bc97c90ddb92025b5aefd4f7ba3cf419bfe2a",
    "colab_type": "text",
    "id": "HSQyNTwZrjIw"
   },
   "source": [
    "### Dataset overview\n",
    "- 12170 rows (days)\n",
    "    - start date: January 4, 1971\n",
    "    - end date: August 25, 2017\n",
    "- 26 columns (23 currencies vs. USD, plus 3 indices*)\n",
    "    - series of different length, rest NaN\n",
    "    - e.g. EUR/USD starts in 1999\n",
    "    - most other main currency pairs in 1971\n",
    "\n",
    "    *(Nominal Broad Dollar Index, Nominal Major Currencies Dollar Index, Nominal Other Important Trading Partners Dollar Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OimUoi4orjIx"
   },
   "source": [
    "#### Create one-day returns\n",
    "\n",
    "- create a separate dataframe for daily returns\n",
    "- skip `NaN`s instead of padding them with zeroes - that would mislead the models with artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1l5hSTMMrjIy"
   },
   "outputs": [],
   "source": [
    "returns = pd.DataFrame(index=prices.index, columns=prices.columns)\n",
    "\n",
    "for col in returns.columns:\n",
    "    returns[col] = prices[col][prices[col].notnull()].pct_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IZqdhlyrjJA"
   },
   "source": [
    "## Modeling and prediction\n",
    "### Parameters\n",
    "#### Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZCX2NGTrjJB"
   },
   "outputs": [],
   "source": [
    "# parameters for data preprocessing\n",
    "train_len = 750\n",
    "trade_len = 250\n",
    "sequence_len = 240\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "# metrics to be evaluated\n",
    "metrics = ['Log loss', 'Accuracy', 'AUC', 'Returns', \n",
    "           'Standard deviation', 'Sharpe ratio', 'Time']\n",
    "\n",
    "# model and currency pair alternatives\n",
    "\n",
    "cell_types = [Dense, SimpleRNN, LSTM, GRU]\n",
    "# cell_types = [Dense, SimpleRNN, CuDNNLSTM, CuDNNGRU]\n",
    "models_str = ['FNN', 'SRNN', 'LSTM', 'GRU']\n",
    "currencies = [\"EUR/USD\", \"GBP/USD\", \"USD/JPY\", \"USD/CHF\", \"USD/CAD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create structure to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZCX2NGTrjJB"
   },
   "outputs": [],
   "source": [
    "# empty dictionary for results: one df for each currency\n",
    "results_dict = {}\n",
    "\n",
    "for curr in currencies:\n",
    "    \n",
    "    # determine number of study periods\n",
    "    study_periods = int((len(returns[curr].dropna()) - train_len) / trade_len)\n",
    "\n",
    "    # create a dataframe to store results\n",
    "    results_dict[curr] = create_results_df(study_periods, metrics)\n",
    "    \n",
    "    results_dict[curr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define index combination\n",
    "h, c = 2, 0\n",
    "\n",
    "# get model and curreny combination to for training/testing\n",
    "hidden_layer_type = cell_types[h]\n",
    "model_str = models_str[h]\n",
    "currency = currencies[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZCX2NGTrjJB"
   },
   "outputs": [],
   "source": [
    "# hyperparameters for model building\n",
    "hidden_layers = 3\n",
    "neurons = 1000\n",
    "dropout = 0\n",
    "\n",
    "# hyperparameters for model training\n",
    "batch_size = 64\n",
    "max_epochs = 100\n",
    "validation_split = 0.2 \n",
    "verbose = 0\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZB5DE61orjJD"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = training_all_periods(currency, hidden_layer_type, hidden_layers, neurons, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40FrxpFCrjJG"
   },
   "source": [
    "#### Aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lneLoOTrrjJH"
   },
   "outputs": [],
   "source": [
    "results_agg_dict = {}\n",
    "\n",
    "results_agg_dict[curr] = aggregate_results_by_model(results_dict[curr], models=[model_str])\n",
    "\n",
    "# # when having trained multiple currency pairs\n",
    "# for c_pair in currencies:\n",
    "#     results_agg_dict[c_pair] = aggregate_results_by_model(results_dict[c_pair], models=models_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9Mc4UwYrjJJ"
   },
   "source": [
    "#### Concatenate to global dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vdh0I0GGrjJK"
   },
   "outputs": [],
   "source": [
    "results_global = pd.concat(results_agg_dict)\n",
    "filename = f'results/results_{model_str}_{curr}_{hidden_layers}hl_{neurons}n_{dropout}d.csv'\n",
    "filename = 'results/final-for-now/results_FNN_CAD.csv'\n",
    "results_global.to_csv(filename)\n",
    "results_global"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FR-tpu-0_loop_nu_data.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
